diff --git a/qwen_api_demo/README.md b/qwen_api_demo/README.md
index abd14bf..a290699 100644
--- a/qwen_api_demo/README.md
+++ b/qwen_api_demo/README.md
@@ -6,18 +6,27 @@
 - `qwen_api_examples.py`：包含 QwenClient 封装与多组参数示例（非流式与流式）。
 - `notes.md`：编写过程中的工作记录。
 
-## 前置条件
-1. 安装依赖：
+## 环境准备（推荐使用 uv）
+> 若尚未安装 [uv](https://github.com/astral-sh/uv)，可先执行 `pip install --upgrade uv`，或参考官方安装脚本。
+
+1. 创建虚拟环境并激活：
+   ```bash
+   uv venv .venv
+   source .venv/bin/activate  # Windows 使用 .venv\Scripts\activate
+   ```
+2. 安装依赖：
    ```bash
-   pip install requests
+   uv pip install requests
    ```
-2. 配置密钥（两者取其一）：
+3. 配置密钥（两者取其一）：
    ```bash
    export DASHSCOPE_API_KEY="你的_API_Key"
    # 或
    export QWEN_API_KEY="你的_API_Key"
    ```
 
+> 如果你已经有现成的虚拟环境，也可以直接在该环境里运行 `uv pip install requests`。
+
 ## 快速运行
 直接运行脚本即可按顺序演示所有示例：
 ```bash
diff --git a/qwen_api_demo/git_diff.patch b/qwen_api_demo/git_diff.patch
index 7b543c3..e69de29 100644
--- a/qwen_api_demo/git_diff.patch
+++ b/qwen_api_demo/git_diff.patch
@@ -1,291 +0,0 @@
-diff --git a/qwen_api_demo/README.md b/qwen_api_demo/README.md
-new file mode 100644
-index 0000000..abd14bf
---- /dev/null
-+++ b/qwen_api_demo/README.md
-@@ -0,0 +1,84 @@
-+# Qwen API Python 示例与推理参数教程
-+
-+本示例基于[阿里云 Model Studio Qwen API 参考](https://help.aliyun.com/zh/model-studio/qwen-api-reference)编写，提供兼容 OpenAI Chat Completions 的 Python 代码，演示不同推理参数的组合与效果，并附上中文说明。
-+
-+## 文件结构
-+- `qwen_api_examples.py`：包含 QwenClient 封装与多组参数示例（非流式与流式）。
-+- `notes.md`：编写过程中的工作记录。
-+
-+## 前置条件
-+1. 安装依赖：
-+   ```bash
-+   pip install requests
-+   ```
-+2. 配置密钥（两者取其一）：
-+   ```bash
-+   export DASHSCOPE_API_KEY="你的_API_Key"
-+   # 或
-+   export QWEN_API_KEY="你的_API_Key"
-+   ```
-+
-+## 快速运行
-+直接运行脚本即可按顺序演示所有示例：
-+```bash
-+python qwen_api_examples.py
-+```
-+
-+## 推理参数教程
-+以下参数均可在 `QwenClient.chat` 或 `QwenClient.stream_chat` 中传入，核心组合示例可在 `qwen_api_examples.py` 中查看。
-+
-+### 1. 模型选择 `model`
-+- 常见取值：`qwen-turbo`（性价比）、`qwen-plus`（平衡）、`qwen-max`（高质量）。
-+- 影响回答质量、时延与费用，脚本中默认值为 `qwen-plus`。
-+
-+### 2. 温度 `temperature`
-+- 作用：控制随机性，范围通常在 0~2。
-+- 低温度（如 0.2）：输出稳定、可复现，适用于精确问答。见 `run_precise_inference`。
-+- 高温度（如 0.95）：输出多样、富有创意，适用于写作。见 `run_creative_writing`。
-+
-+### 3. 采样范围 `top_p`
-+- 作用：核采样限制概率质量总和，取值 0~1。
-+- 较低值（如 0.6~0.8）可减少离谱答案；较高值（0.9）提升想象力。
-+- 常与 `temperature` 联合调节，避免同时取极端值。
-+
-+### 4. 最大长度 `max_tokens`
-+- 作用：限制返回 Token 数，防止输出过长或超额计费。
-+- 说明：包含所有 role 的最终回复长度，不含历史消息。
-+- 示例：`run_precise_inference` 与 `run_creative_writing` 将其限制在 128~256 之间。
-+
-+### 5. 停止词 `stop`
-+- 作用：当模型生成的文本包含列表中的任一字符串时立即终止。
-+- 用法：传入字符串数组，例如 `stop=["内容："]`。
-+- 示例：`run_stop_words_example` 演示如何用 stop 词控制格式。
-+
-+### 6. 惩罚项 `presence_penalty` 与 `frequency_penalty`
-+- 作用：抑制重复、鼓励覆盖新主题，典型取值范围 -2.0 ~ 2.0。
-+- `presence_penalty`：惩罚已出现过的主题，适合获得更多新要点。
-+- `frequency_penalty`：惩罚重复词汇，减少啰嗦，示例中设置为 `0.5`。
-+
-+### 7. 随机种子 `seed`
-+- 作用：在相同温度与输入下获得更可复现的结果。
-+- 示例：`run_precise_inference` 使用 `seed=42`，方便回放调试。
-+
-+### 8. 流式推理 `stream`
-+- 作用：实时获取模型输出，适合聊天体验或渐进式渲染。
-+- 用法：将 `stream=True` 传入请求，并按行解析 `data: {...}` 事件。
-+- 示例：`run_streaming_demo` 将分段增量组装成完整文本。
-+
-+## 如何自定义请求
-+在 `QwenClient.chat` 的调用中添加或修改参数即可：
-+```python
-+client.chat(
-+    messages=[{"role": "user", "content": "解释温度和 top_p 的差异"}],
-+    model="qwen-turbo",
-+    temperature=0.3,
-+    top_p=0.7,
-+    max_tokens=150,
-+)
-+```
-+如需进一步控制（例如工具调用、响应格式约束 JSON 等），可根据官方文档添加对应字段，客户端会将参数透传给接口。
-+
-+## 注意事项
-+- 网络请求失败时 `QwenClient` 会抛出异常，可根据业务需要在外层捕获并重试。
-+- 生产环境应妥善保管密钥，并为不同场景设置合理的超时与重试策略。
-+- 若需要 OpenAI SDK 兼容调用，可将 `base_url` 指向阿里云兼容地址，并在同一写法下使用官方 SDK。
-diff --git a/qwen_api_demo/notes.md b/qwen_api_demo/notes.md
-new file mode 100644
-index 0000000..cdd0519
---- /dev/null
-+++ b/qwen_api_demo/notes.md
-@@ -0,0 +1,7 @@
-+# 工作笔记
-+
-+## 2024-XX-XX
-+- 创建 qwen_api_demo 目录并初始化笔记。
-+- 准备根据阿里云 Model Studio Qwen API 文档编写 Python 示例，覆盖推理参数与调用方式。
-+- 编写 `qwen_api_examples.py`，包含 QwenClient 封装、非流式与流式推理、温度/top_p/stop/seed 等参数示例。
-+- 撰写 README.md，整理运行步骤与推理参数中文教程。
-diff --git a/qwen_api_demo/qwen_api_examples.py b/qwen_api_demo/qwen_api_examples.py
-new file mode 100644
-index 0000000..897a13a
---- /dev/null
-+++ b/qwen_api_demo/qwen_api_examples.py
-@@ -0,0 +1,182 @@
-+"""
-+基于阿里云 Model Studio Qwen 接口的 Python 示例。
-+
-+- 采用与 OpenAI 兼容的 `/compatible-mode/v1/chat/completions` 端点。
-+- 通过不同函数演示常用推理参数的组合和差异。
-+- 所有示例都使用中文注释，便于快速对照官方文档。
-+"""
-+
-+from __future__ import annotations
-+
-+import json
-+import os
-+from typing import Any, Dict, Iterable, List
-+
-+import requests
-+
-+
-+class QwenClient:
-+    """简单的 Qwen HTTP 客户端封装。
-+
-+    参数说明:
-+        api_key: 从环境变量或配置中心获得的 Bearer Token。
-+        base_url: 兼容 OpenAI 的 Chat Completions 端点。
-+    """
-+
-+    def __init__(self, api_key: str | None = None, *, base_url: str | None = None) -> None:
-+        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY") or os.getenv("QWEN_API_KEY")
-+        if not self.api_key:
-+            raise RuntimeError("请在环境变量 DASHSCOPE_API_KEY 或 QWEN_API_KEY 中配置 API Key")
-+
-+        # 文档提供的兼容模式地址，适合大多数聊天/推理调用。
-+        self.base_url = base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
-+
-+        self.session = requests.Session()
-+        self.session.headers.update(
-+            {
-+                "Authorization": f"Bearer {self.api_key}",
-+                "Content-Type": "application/json",
-+            }
-+        )
-+
-+    def chat(self, messages: List[Dict[str, str]], **params: Any) -> Dict[str, Any]:
-+        """非流式调用，返回完整 JSON。
-+
-+        常用参数(详见官方参考):
-+            model: 指定模型名称，如 `qwen-plus`、`qwen-max`、`qwen-turbo` 等。
-+            temperature/top_p: 控制随机性与采样范围。
-+            max_tokens: 限制回复长度，防止超长输出。
-+            stop: 提前终止生成的停止词列表。
-+            presence_penalty/frequency_penalty: 惩罚重复，鼓励多样性。
-+            seed: 设定随机种子，便于结果复现。
-+        """
-+
-+        payload = {"model": params.pop("model", "qwen-plus"), "messages": messages, **params}
-+        response = self.session.post(self.base_url, json=payload, timeout=30)
-+        response.raise_for_status()
-+        return response.json()
-+
-+    def stream_chat(self, messages: List[Dict[str, str]], **params: Any) -> Iterable[Dict[str, Any]]:
-+        """流式推理示例，逐段读取返回的 SSE 数据行。
-+
-+        - 设置 `stream=True` 后，接口会以 `data: ...` 的形式持续返回。
-+        - 本函数演示如何解析每行 JSON，并在遇到 `[DONE]` 时结束。
-+        """
-+
-+        payload = {
-+            "model": params.pop("model", "qwen-plus"),
-+            "messages": messages,
-+            "stream": True,
-+            **params,
-+        }
-+        with self.session.post(self.base_url, json=payload, stream=True, timeout=30) as response:
-+            response.raise_for_status()
-+            for line in response.iter_lines():
-+                if not line:
-+                    continue
-+                if line.strip() == b"data: [DONE]":
-+                    break
-+                if not line.startswith(b"data: "):
-+                    continue
-+                chunk = json.loads(line[len(b"data: ") :])
-+                yield chunk
-+
-+
-+# ============================ 示例调用区域 ============================
-+
-+def run_minimal_chat(client: QwenClient) -> Dict[str, Any]:
-+    """最小化示例：默认模型、默认温度，关注正确性和速度。"""
-+
-+    messages = [
-+        {"role": "system", "content": "你是一位乐于解释的中文助理"},
-+        {"role": "user", "content": "简要介绍下阿里云 Model Studio 是什么？"},
-+    ]
-+    return client.chat(messages)
-+
-+
-+def run_precise_inference(client: QwenClient) -> Dict[str, Any]:
-+    """精确推理：降低随机性，限制回复长度。"""
-+
-+    messages = [{"role": "user", "content": "给出 3 条适合初学者的 prompt 编写技巧。"}]
-+    return client.chat(
-+        messages,
-+        model="qwen-turbo",
-+        temperature=0.2,  # 越低越趋向确定性
-+        top_p=0.7,  # 限制采样范围，避免离谱答案
-+        max_tokens=256,
-+        presence_penalty=0.0,
-+        frequency_penalty=0.5,  # 惩罚重复，减少啰嗦
-+        seed=42,  # 固定随机种子，便于复现
-+    )
-+
-+
-+def run_creative_writing(client: QwenClient) -> Dict[str, Any]:
-+    """创意写作：提高温度和采样多样性。"""
-+
-+    messages = [
-+        {"role": "system", "content": "请用活泼的语气"},
-+        {"role": "user", "content": "写一首关于云计算的四行短诗"},
-+    ]
-+    return client.chat(
-+        messages,
-+        model="qwen-plus",
-+        temperature=0.95,
-+        top_p=0.9,
-+        max_tokens=128,
-+    )
-+
-+
-+def run_stop_words_example(client: QwenClient) -> Dict[str, Any]:
-+    """使用 stop 词提前截断输出，便于格式控制。"""
-+
-+    messages = [
-+        {
-+            "role": "user",
-+            "content": "按照“标题：\n内容：”的格式输出一段介绍，生成时遇到“内容：”后立即停止。",
-+        }
-+    ]
-+    return client.chat(
-+        messages,
-+        stop=["内容："],
-+        max_tokens=200,
-+    )
-+
-+
-+def run_streaming_demo(client: QwenClient) -> List[str]:
-+    """演示流式推理，将模型逐步输出的内容收集为字符串列表。"""
-+
-+    messages = [
-+        {"role": "user", "content": "逐步解释大模型推理的温度与 top_p 的区别。"}
-+    ]
-+    chunks: List[str] = []
-+    for event in client.stream_chat(messages, temperature=0.6, top_p=0.8):
-+        # 每个 event 的结构与非流式类似，但内容在 choices 中分段出现
-+        delta = event.get("choices", [{}])[0].get("delta", {}).get("content")
-+        if delta:
-+            chunks.append(delta)
-+    return chunks
-+
-+
-+if __name__ == "__main__":
-+    # 提示：正式使用前请将 DASHSCOPE_API_KEY / QWEN_API_KEY 配置到环境变量。
-+    client = QwenClient()
-+
-+    print("\n[1] 最小化调用示例")
-+    minimal = run_minimal_chat(client)
-+    print(json.dumps(minimal, ensure_ascii=False, indent=2))
-+
-+    print("\n[2] 精确推理示例")
-+    precise = run_precise_inference(client)
-+    print(json.dumps(precise, ensure_ascii=False, indent=2))
-+
-+    print("\n[3] 创意写作示例")
-+    creative = run_creative_writing(client)
-+    print(json.dumps(creative, ensure_ascii=False, indent=2))
-+
-+    print("\n[4] stop 词截断示例")
-+    truncated = run_stop_words_example(client)
-+    print(json.dumps(truncated, ensure_ascii=False, indent=2))
-+
-+    print("\n[5] 流式推理示例 (打印组装后的文本)")
-+    stream_chunks = run_streaming_demo(client)
-+    print("".join(stream_chunks))
diff --git a/qwen_api_demo/notes.md b/qwen_api_demo/notes.md
index cdd0519..6545c8e 100644
--- a/qwen_api_demo/notes.md
+++ b/qwen_api_demo/notes.md
@@ -5,3 +5,8 @@
 - 准备根据阿里云 Model Studio Qwen API 文档编写 Python 示例，覆盖推理参数与调用方式。
 - 编写 `qwen_api_examples.py`，包含 QwenClient 封装、非流式与流式推理、温度/top_p/stop/seed 等参数示例。
 - 撰写 README.md，整理运行步骤与推理参数中文教程。
+
+## 2025-12-09
+- 根据追加要求，补充在 README 中使用 uv 创建虚拟环境和安装依赖的步骤。
+- 检查示例代码无需改动，仅完善文档即可满足环境管理需求。
+- 计划更新 git_diff.patch 以记录本次文档调整。
diff --git a/qwen_api_demo/openai_qwen_demo/README.md b/qwen_api_demo/openai_qwen_demo/README.md
index ad8b298..20e398d 100644
--- a/qwen_api_demo/openai_qwen_demo/README.md
+++ b/qwen_api_demo/openai_qwen_demo/README.md
@@ -6,12 +6,19 @@
 - 思考/推理型模型；
 - 关键推理参数（温度、Top-P、停止词、种子等）。
 
-## 环境准备
-1. 安装依赖：
+## 环境准备（推荐使用 uv）
+> 如未安装 [uv](https://github.com/astral-sh/uv)，可先运行 `pip install --upgrade uv`。若已有虚拟环境，可直接在环境内使用 `uv pip install openai`。
+
+1. 创建虚拟环境并激活：
+   ```bash
+   uv venv .venv
+   source .venv/bin/activate  # Windows 使用 .venv\Scripts\activate
+   ```
+2. 安装依赖：
    ```bash
-   pip install openai
+   uv pip install openai
    ```
-2. 配置环境变量（至少需要 API Key）：
+3. 配置环境变量（至少需要 API Key）：
    ```bash
    export QWEN_OPENAI_API_KEY="你的DashScope或千问API Key"
    export QWEN_OPENAI_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"  # 可选，默认已设置
diff --git a/qwen_api_demo/openai_qwen_demo/notes.md b/qwen_api_demo/openai_qwen_demo/notes.md
index 662839d..83cadc3 100644
--- a/qwen_api_demo/openai_qwen_demo/notes.md
+++ b/qwen_api_demo/openai_qwen_demo/notes.md
@@ -3,3 +3,7 @@
 - 创建 `openai_qwen_demo` 目录，准备用 OpenAI SDK 演示调用通义千问（文本、多模态、思考模型）。
 - 编写 `openai_qwen_examples.py`，涵盖非流式、流式、多模态与思考模型示例，全部使用 OpenAI SDK。
 - 撰写 README，整理环境变量、运行方式、参数要点和多模态/思考模型使用提示。
+
+## 2025-12-09
+- 补充使用 uv 管理虚拟环境与依赖的说明，与 OpenAI SDK 示例保持一致。
+- 无需修改示例脚本，重点完善环境准备文档。
